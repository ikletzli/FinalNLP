{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class VisionTransformer(nn.Module):\n",
    "#     \"\"\"Vision Transformer as per https://arxiv.org/abs/2010.11929.\"\"\"\n",
    "\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         image_size: int,\n",
    "#         patch_size: int,\n",
    "#         num_layers: int,\n",
    "#         num_heads: int,\n",
    "#         hidden_dim: int,\n",
    "#         mlp_dim: int,\n",
    "#         dropout: float = 0.0,\n",
    "#         attention_dropout: float = 0.0,\n",
    "#         num_classes: int = 1000,\n",
    "#         representation_size: Optional[int] = None,\n",
    "#         norm_layer: Callable[..., torch.nn.Module] = partial(nn.LayerNorm, eps=1e-6),\n",
    "#         conv_stem_configs: Optional[List[ConvStemConfig]] = None,\n",
    "#     ):\n",
    "import torchvision\n",
    "from torchvision.models import VisionTransformer\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from replacer import replace_linears_in_pytorch_model\n",
    "from bitnet158 import BitLinear158B\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 60000 instances\n",
      "Validation set has 10000 instances\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [torchvision.transforms.Grayscale(num_output_channels=3),\n",
    "     transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Create datasets for training & validation, download if necessary\n",
    "training_set = torchvision.datasets.MNIST('./data', train=True, transform=transform, download=True)\n",
    "validation_set = torchvision.datasets.MNIST('./data', train=False, transform=transform, download=True)\n",
    "\n",
    "# Create data loaders for our datasets; shuffle for training, not for validation\n",
    "training_loader = torch.utils.data.DataLoader(training_set, batch_size=4, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=4, shuffle=False)\n",
    "\n",
    "print('Training set has {} instances'.format(len(training_set)))\n",
    "print('Validation set has {} instances'.format(len(validation_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 122)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Helper function for inline image display\n",
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    print(npimg.shape)\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "dataiter = iter(training_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# Create a grid from the images and show them\n",
    "img_grid = torchvision.utils.make_grid(images)\n",
    "matplotlib_imshow(img_grid, one_channel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = VisionTransformer(image_size=28, patch_size=4, num_layers=8, num_heads=2, hidden_dim=1024, mlp_dim=2048, num_classes=10)\n",
    "model = VisionTransformer(image_size=28, patch_size=4, num_layers=4, num_heads=2, hidden_dim=20, mlp_dim=20, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0855, 0.0572, 0.8516, 0.8469, 0.8856, 0.2716, 0.1229, 0.1822, 0.8401,\n",
      "         0.1057],\n",
      "        [0.7339, 0.8309, 0.6936, 0.8730, 0.1006, 0.2502, 0.3082, 0.8046, 0.9776,\n",
      "         0.3407],\n",
      "        [0.7330, 0.2695, 0.5288, 0.0896, 0.8581, 0.1871, 0.8165, 0.2474, 0.5771,\n",
      "         0.3944],\n",
      "        [0.6164, 0.9046, 0.3616, 0.3308, 0.5370, 0.0726, 0.9725, 0.4090, 0.9741,\n",
      "         0.5671]])\n",
      "tensor([1, 5, 3, 7])\n",
      "Total loss for this batch: 2.6614456176757812\n"
     ]
    }
   ],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# NB: Loss functions expect data in batches, so we're creating batches of 4\n",
    "# Represents the model's confidence in each of the 10 classes for a given input\n",
    "dummy_outputs = torch.rand(4, 10)\n",
    "# Represents the correct class among the 10 being tested\n",
    "dummy_labels = torch.tensor([1, 5, 3, 7])\n",
    "\n",
    "print(dummy_outputs)\n",
    "print(dummy_labels)\n",
    "\n",
    "loss = loss_fn(dummy_outputs, dummy_labels)\n",
    "print('Total loss for this batch: {}'.format(loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers specified in the torch.optim package\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in enumerate(training_loader):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:\n",
    "            last_loss = running_loss / 1000 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(training_loader) + i + 1\n",
    "            #add wandb\n",
    "            #tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 1000 loss: 2.056946685731411\n",
      "  batch 2000 loss: 1.7297265878021717\n",
      "  batch 3000 loss: 1.6282375343441964\n",
      "  batch 4000 loss: 1.5318664744496346\n",
      "  batch 5000 loss: 1.3643767423927784\n",
      "  batch 6000 loss: 1.1666741925179958\n",
      "  batch 7000 loss: 1.0139263101741671\n",
      "  batch 8000 loss: 0.9546235912069678\n",
      "  batch 9000 loss: 0.8750821129754186\n",
      "  batch 10000 loss: 0.789939449608326\n",
      "  batch 11000 loss: 0.7291913877855987\n",
      "  batch 12000 loss: 0.6726334815919399\n",
      "  batch 13000 loss: 0.6295736635399517\n",
      "  batch 14000 loss: 0.6059867246374487\n",
      "  batch 15000 loss: 0.572288870062679\n",
      "LOSS train 0.572288870062679 valid 0.49554046988487244\n"
     ]
    }
   ],
   "source": [
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "epoch_number = 0\n",
    "\n",
    "EPOCHS = 1\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch(epoch_number)\n",
    "\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    # Set the model to evaluation mode, disabling dropout and using population\n",
    "    # statistics for batch normalization.\n",
    "    model.eval()\n",
    "\n",
    "    # Disable gradient computation and reduce memory consumption.\n",
    "    with torch.no_grad():\n",
    "        for i, vdata in enumerate(validation_loader):\n",
    "            vinputs, vlabels = vdata\n",
    "            voutputs = model(vinputs)\n",
    "            vloss = loss_fn(voutputs, vlabels)\n",
    "            running_vloss += vloss\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    # # Log the running loss averaged per batch\n",
    "    # # for both training and validation\n",
    "    # writer.add_scalars('Training vs. Validation Loss',\n",
    "    #                 { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "    #                 epoch_number + 1)\n",
    "    # writer.flush()\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8407\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    total = 0\n",
    "    num_correct = 0\n",
    "    for i, vdata in enumerate(validation_loader):\n",
    "        vinputs, vlabels = vdata\n",
    "        voutputs = model(vinputs)\n",
    "        pred = torch.argmax(voutputs, dim=1)\n",
    "        num_equal = torch.sum(vlabels == pred)\n",
    "        num_correct += num_equal.item()\n",
    "        total += 4\n",
    "    \n",
    "    print(num_correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if parameter.requires_grad:\n",
    "            params = parameter.numel()\n",
    "            total_params += params\n",
    "    return total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_bit_linear_layer_params(model, BitLinear):\n",
    "    total_params = 0\n",
    "    for name, module in model.named_children():\n",
    "        if isinstance(module, BitLinear):\n",
    "            total_params += count_parameters(module)\n",
    "        else:\n",
    "            # Recursively apply to child modules\n",
    "            total_params += count_bit_linear_layer_params(module, BitLinear)\n",
    "    \n",
    "    return total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_bytes(model, BitLinear):\n",
    "    all_params = count_parameters(model)\n",
    "    bit_linear_params = count_bit_linear_layer_params(model, BitLinear)\n",
    "    non_bit_params = all_params - bit_linear_params\n",
    "    bits = bit_linear_params * 2 + non_bit_params * 32\n",
    "    return math.ceil(bits / 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "269254696\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "111806979\n"
     ]
    }
   ],
   "source": [
    "print(count_bytes(model, BitLinear158B))\n",
    "replace_linears_in_pytorch_model(model, BitLinear158B)\n",
    "print(\"\\n\\n\\n\\n\\n\")\n",
    "print(count_bytes(model, BitLinear158B))\n",
    "\n",
    "m = BitLinear158B(20, 30)\n",
    "\n",
    "input = torch.randn(128, 20)\n",
    "output = m(input)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import lightning as L\n",
    "import finalnlp\n",
    "from finalnlp.gpt.gpt_model import GPT\n",
    "from finalnlp.gpt.nb.share import LitGPT, SortDataset, eval_split, length, num_digits, train_dataset, test_dataset, model_type, vocab_size, block_size, val_check_interval, max_steps\n",
    "from finalnlp.replacer import replace_linears_in_pytorch_model\n",
    "from finalnlp import bitnet1\n",
    "from finalnlp import bitnet158\n",
    "from lightning.pytorch import loggers as pl_loggers\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 -1\n",
      "3 -1\n",
      "0 -1\n",
      "2 -1\n",
      "2 -1\n",
      "3 -1\n",
      "0 -1\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 2\n",
      "2 2\n",
      "2 2\n",
      "2 3\n",
      "3 3\n"
     ]
    }
   ],
   "source": [
    "x, y = train_dataset[0]\n",
    "for a, b in zip(x,y):\n",
    "    print(int(a),int(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plain Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 0.09M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcandrewlee14\u001b[0m (\u001b[33mandrews-org\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20240412_143538-9qwnypsl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/andrews-org/lightning_logs/runs/9qwnypsl/workspace' target=\"_blank\">GPT-Sort-Problem-len8gpt-nano</a></strong> to <a href='https://wandb.ai/andrews-org/lightning_logs' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/andrews-org/lightning_logs' target=\"_blank\">https://wandb.ai/andrews-org/lightning_logs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/andrews-org/lightning_logs/runs/9qwnypsl/workspace' target=\"_blank\">https://wandb.ai/andrews-org/lightning_logs/runs/9qwnypsl/workspace</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type | Params\n",
      "-------------------------------\n",
      "0 | model | GPT  | 86.0 K\n",
      "-------------------------------\n",
      "86.0 K    Trainable params\n",
      "0         Non-trainable params\n",
      "86.0 K    Total params\n",
      "0.344     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b479985fd411469aa3bb6aee5360ebd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d3267789aab406f9f2bbe17499ab1ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95e8008378224064ac6605c307f1c548",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e41ce6dee9934a04835f104226a6aac2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2170e00db264a9fae42ccab17945670",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1c0b6045bde43f9848092dea83e2bd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4de3faf9f1ef4dccbc23e6e1ca0b291e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5573e909d9ad42fa9be4a6ca30575cb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e8a6fcdb5054b86a55cb1a26fbcbcfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "609e03e43a1740e889c400920b40eb00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcc01c353cf343cd93712c3c312b8d4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20cd9e30205849e1b367f9900fe2d4b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=10000` reached.\n"
     ]
    }
   ],
   "source": [
    "# autoencoder = LitAutoEncoder(Encoder(), Decoder())\n",
    "L.seed_everything(42, workers=True)\n",
    "model_config = GPT.get_default_config()\n",
    "model_config.model_type = model_type\n",
    "model_config.vocab_size = vocab_size\n",
    "model_config.block_size = block_size\n",
    "model = LitGPT(model_config, train_dataset.length)\n",
    "\n",
    "wandb_logger = pl_loggers.WandbLogger(\"GPT-Sort-Problem-len\" + str(length) + str(model_type))\n",
    "wandb_logger.experiment.config.update(model_config)\n",
    "wandb_logger.experiment.config.update({\"problem\": \"sort\", \"linear_replacer\": \"none\"})\n",
    "wandb_logger.experiment.config.update({\"length\": length, \"num_digits\": num_digits})\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    # callbacks=[EarlyStopping(monitor=\"train_loss\", mode=\"min\")],\n",
    "    logger=wandb_logger,\n",
    "    max_steps=max_steps,\n",
    "    val_check_interval=val_check_interval,\n",
    ")\n",
    "wandb_logger.watch(model)\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "trainer.fit(\n",
    "    model=model,\n",
    "    train_dataloaders=DataLoader(train_dataset, num_workers=15),\n",
    "    val_dataloaders=DataLoader(test_dataset, num_workers=15),    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81ee630009d14ea99cdf22782e12daf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "     Validate metric           DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "         val_acc             0.972000002861023\n",
      "        val_loss           0.031101757660508156\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'val_loss': 0.031101757660508156, 'val_acc': 0.972000002861023}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.validate(model=model, dataloaders=DataLoader(test_dataset, num_workers=15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT claims that [3, 1, 1, 3, 2, 2, 3, 3] sorted is [1, 1, 2, 2, 2, 3, 3, 3] but gt is [1, 1, 2, 2, 3, 3, 3, 3]\n",
      "GPT claims that [3, 3, 0, 2, 2, 3, 3, 1] sorted is [0, 1, 2, 2, 2, 3, 3, 3] but gt is [0, 1, 2, 2, 3, 3, 3, 3]\n",
      "GPT claims that [3, 2, 2, 3, 3, 3, 3, 1] sorted is [1, 2, 2, 2, 3, 3, 3, 3] but gt is [1, 2, 2, 3, 3, 3, 3, 3]\n",
      "train final score: 947/1000 = 94.70% correct\n",
      "GPT claims that [3, 2, 3, 3, 1, 2, 2, 3] sorted is [1, 2, 2, 2, 2, 3, 3, 3] but gt is [1, 2, 2, 2, 3, 3, 3, 3]\n",
      "GPT claims that [3, 3, 1, 2, 0, 2, 3, 3] sorted is [0, 1, 2, 2, 2, 3, 3, 3] but gt is [0, 1, 2, 2, 3, 3, 3, 3]\n",
      "GPT claims that [0, 0, 2, 0, 0, 0, 1, 3] sorted is [0, 0, 0, 0, 1, 2, 2, 3] but gt is [0, 0, 0, 0, 0, 1, 2, 3]\n",
      "test final score: 239/250 = 95.60% correct\n"
     ]
    }
   ],
   "source": [
    "# run a lot of examples from both train and test through the model and verify the output correctness\n",
    "with torch.no_grad():\n",
    "    train_score = eval_split(model, trainer, train_dataset, test_dataset, 'train', max_batches=50)\n",
    "    test_score  = eval_split(model, trainer, train_dataset, test_dataset, 'test',  max_batches=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input sequence  : [[1, 3, 3, 0, 2, 1, 3, 2, 0, 1, 1, 2, 2, 3, 3]]\n",
      "predicted sorted: [[0, 1, 1, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]]\n",
      "gt sort         : [0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 3]\n",
      "matches         : False\n"
     ]
    }
   ],
   "source": [
    "# let's run a random given sequence through the model as well\n",
    "n = train_dataset.length\n",
    "inp = torch.tensor([[0, 0, 2, 1, 0, 1, 3, 4, 5, 6]], dtype=torch.long)\n",
    "inp, y = train_dataset[0]\n",
    "inp = inp.unsqueeze(dim=0)\n",
    "# assert inp[0].nelement() == n\n",
    "with torch.no_grad():\n",
    "    cat = model.model.generate(inp, n, do_sample=False)\n",
    "sol = torch.sort(inp[0])[0]\n",
    "sol_candidate = cat[:, n:]\n",
    "print('input sequence  :', inp.tolist())\n",
    "print('predicted sorted:', sol_candidate.tolist())\n",
    "print('gt sort         :', sol.tolist())\n",
    "print('matches         :', bool((sol == sol_candidate).all()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
